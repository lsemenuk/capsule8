// Copyright 2017 Capsule8, Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package sensor

// This file implements a process information cache that uses a sensor's
// system-global EventMonitor to keep it up-to-date. The cache also monitors
// for runc container starts to identify the containerID for a given PID
// namespace. Process information gathered by the cache may be retrieved via
// the LookupTask and LookupTaskAndLeader methods.
//
// glog levels used:
//   10 = cache operation level tracing for debugging

import (
	"errors"
	"fmt"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"time"
	"unsafe"

	"github.com/capsule8/capsule8/pkg/config"
	"github.com/capsule8/capsule8/pkg/expression"
	"github.com/capsule8/capsule8/pkg/sys"
	"github.com/capsule8/capsule8/pkg/sys/perf"

	"github.com/golang/glog"
	"golang.org/x/sys/unix"
)

// ProcessExecEventTypes defines the field types that can be used with filters
// on process exec telemetry events.
var ProcessExecEventTypes = expression.FieldTypeMap{
	"filename": expression.ValueTypeString,
	"cwd":      expression.ValueTypeString,
}

// ProcessExitEventTypes defines the field types that can be used with filters
// on process exit telemetry events.
var ProcessExitEventTypes = expression.FieldTypeMap{
	"code":             expression.ValueTypeSignedInt32,
	"exit_status":      expression.ValueTypeUnsignedInt32,
	"exit_signal":      expression.ValueTypeUnsignedInt32,
	"exit_core_dumped": expression.ValueTypeBool,
}

// ProcessForkEventTypes defines the field types that can be used with filters
// on process fork telemetry events.
var ProcessForkEventTypes = expression.FieldTypeMap{
	"fork_child_pid":   expression.ValueTypeSignedInt32,
	"fork_child_id":    expression.ValueTypeString,
	"fork_clone_flags": expression.ValueTypeUnsignedInt64,
	"fork_stack_start": expression.ValueTypeUnsignedInt64,
	"cwd":              expression.ValueTypeString,
}

// ProcessUpdateEventTypes defines the field types that can be used with
// filters on process update telemetry events.
var ProcessUpdateEventTypes = expression.FieldTypeMap{
	"cwd": expression.ValueTypeString,
}

// ProcessExecTelemetryEvent is a telemetry event generated by the process
// exec event source.
type ProcessExecTelemetryEvent struct {
	TelemetryEventData

	Filename    string
	CommandLine []string
	CWD         string
}

// CommonTelemetryEventData returns the telemtry event data common to all
// telemetry events for a chargen telemetry event.
func (e ProcessExecTelemetryEvent) CommonTelemetryEventData() TelemetryEventData {
	return e.TelemetryEventData
}

// ProcessExitTelemetryEvent is a telemetry event generated by the process
// exit event source.
type ProcessExitTelemetryEvent struct {
	TelemetryEventData

	ExitCode       int32
	ExitStatus     uint32
	ExitSignal     uint32
	ExitCoreDumped bool
}

// CommonTelemetryEventData returns the telemtry event data common to all
// telemetry events for a chargen telemetry event.
func (e ProcessExitTelemetryEvent) CommonTelemetryEventData() TelemetryEventData {
	return e.TelemetryEventData
}

// ProcessForkTelemetryEvent is a telemetry event generated by the process
// fork event source.
type ProcessForkTelemetryEvent struct {
	TelemetryEventData

	ChildPID       int32
	ChildProcessID string
	CloneFlags     uint64
	StackStart     uint64
	CWD            string
}

// CommonTelemetryEventData returns the telemtry event data common to all
// telemetry events for a chargen telemetry event.
func (e ProcessForkTelemetryEvent) CommonTelemetryEventData() TelemetryEventData {
	return e.TelemetryEventData
}

// ProcessUpdateTelemetryEvent is a telemetry event generated by the process
// update event source.
type ProcessUpdateTelemetryEvent struct {
	TelemetryEventData

	CWD string
}

// CommonTelemetryEventData returns the telemtry event data common to all
// telemetry events for a chargen telemetry event.
func (e ProcessUpdateTelemetryEvent) CommonTelemetryEventData() TelemetryEventData {
	return e.TelemetryEventData
}

const taskReuseThreshold = int64(10 * time.Millisecond)

const (
	commitCredsAddress = "commit_creds"
	commitCredsArgs    = "uid=+4(%di):u32 gid=+8(%di):u32 " +
		"suid=+12(%di):u32 sgid=+16(%di):u32 " +
		"euid=+20(%di):u32 egid=+24(%di):u32 " +
		"fsuid=+28(%di):u32 fsgid=+32(%di):u32"

	// Kernel versions 3.16 through 4.16 should all work with this symbol
	// and fetchargs. Older kernels will need to use attach_task_by_pid,
	// below.
	//
	// static ssize_t __cgroup_procs_write(struct kernfs_open_file *of,
	//      char *buf, size_t nbytes, loff_t off, bool threadgroup)
	// container_id comes from of->file->f_path.dentry->d_parent->d_name.name
	cgroupProcsWriteAddress = "__cgroup_procs_write"
	cgroupProcsWriteArgs    = "container_id=+0(+40(+24(+24(+8(%di))))):string " +
		"buf=+0(%si):string threadgroup=%r8:s32"

	cgroup1ProcsWriteAddress = "__cgroup1_procs_write"
	cgroup1ProcsWriteArgs    = "container_id=+0(+40(+24(+24(+8(%di))))):string " +
		"buf=+0(%si):string threadgroup=%r8:s32"

	// static int attach_task_by_pid(struct cgroup *cgrp, u64 pid, bool threadgroup)
	attachTaskByPid      = "attach_task_by_pid"
	attachTaskByPidArgs1 = "container_id=+0(+56(+56(%di))):string pid=%si:u64 threadgroup=%dx:u32" // RHEL 6: 2.6.32
	attachTaskByPidArgs2 = "container_id=+0(+40(+72(%di))):string pid=%si:u64 threadgroup=%dx:u32" // 3.5.x-3.14.x
	attachTaskByPidArgs3 = "container_id=+0(+16(+64(%di))):string pid=%si:u64 threadgroup=%dx:u32" // 3.15.x

	execveArgCount = 6

	doExecvefileAddress              = "__do_execve_file"
	doExecvefileArgs                 = "filename=+0(+0(%si)):string "
	doExecveatCommonAddress          = "do_execveat_common"
	doExecveatCommonArgs             = "filename=+0(+0(%si)):string "
	doExecveCommonAddress            = "do_execve_common"
	doExecveCommonConstCharArgs      = "filename=+0(%di):string "
	doExecveCommonStructFilenameArgs = "filename=+0(+0(%di)):string "
	doExecveAddress                  = "do_execve"
	doExecveArgs                     = "filename=+0(%di):string "

	doExitAddress = "do_exit"
	doExitArgs    = "code=%di:s64"

	doForkAddress   = "do_fork"
	doForkFetchargs = "clone_flags=%di:u64 stack_start=%si:u64"

	// For kernels where we have offsets into struct task_struct, we will
	// use different kprobes for tracking forks. This is mainly so that we
	// can get the task start_time as efficiently as possible, because
	// hitting /proc/pid/status is really slow. Unfortunately there's not
	// a single kprobe that is good for most kernel versions, so we have to
	// use two and combine the information. We'll get clone_flags from the
	// call to do_fork (doForkAddress, doForkFetchargs above) and we'll get
	// the resulting task struct for the new task from the call to
	// wake_up_new_task. In older kernels, clone_flags is also passed to
	// that function, but there's not a good way to make sure that we have
	// a version of the kernel that we can reliably get that from, so we
	// will just always split the calls to make things consistent for all
	// kernel versions.
	wakeUpNewTaskAddress                  = "wake_up_new_task"
	wakeUpNewTaskRealStartTime16Fetchargs = "child_pid=+PID_OFFSET(%di):s32 start_time_sec=+START_TIME_SEC_OFFSET(%di):s64 start_time_nsec=+START_TIME_NSEC_OFFSET(%di):s64"
	wakeUpNewTaskRealStartTime8Fetchargs  = "child_pid=+PID_OFFSET(%di):s32 start_time_sec=+START_TIME_SEC_OFFSET(%di):s64"

	// This is used for tracking current working directory. It's a
	// a kretprobe that's used to trigger a lookup in /proc to get the
	// needed data.
	doSetFsPwd = "set_fs_pwd"
)

var execveArgString = []string{"argv0", "argv1", "argv2", "argv3", "argv4", "argv5"}

// Cred contains task credential information
type Cred struct {
	// UID is the real UID
	UID uint32
	// GID is the real GID
	GID uint32
	// EUID is the effective UID
	EUID uint32
	// EGID is the effective GID
	EGID uint32
	// SUID is the saved UID
	SUID uint32
	// SGID is the saved GID
	SGID uint32
	// FSUID is the UID for filesystem operations
	FSUID uint32
	// FSGID is the GID for filesystem operations
	FSGID uint32
}

var rootCredentials = Cred{}

func newCredentials(
	uid, euid, suid, fsuid uint32,
	gid, egid, sgid, fsgid uint32,
) *Cred {
	if uid+euid+suid+fsuid+gid+egid+sgid+fsgid == 0 {
		return &rootCredentials
	}

	return &Cred{
		UID:   uid,
		GID:   gid,
		EUID:  euid,
		EGID:  egid,
		SUID:  suid,
		SGID:  sgid,
		FSUID: fsuid,
		FSGID: fsgid,
	}
}

const cloneEventThreshold = uint64(100 * time.Millisecond)

type cloneEvent struct {
	timestamp uint64

	// Set by the parent
	cloneFlags uint64
	stackStart uint64

	// Set by the child
	childPid  int
	childComm string
}

func (c *cloneEvent) isExpired(t uint64) bool {
	if c == nil {
		return true
	}
	var delta uint64
	if c.timestamp < t {
		delta = t - c.timestamp
	} else {
		delta = c.timestamp - t
	}
	return delta > cloneEventThreshold
}

// Task represents a schedulable task. All Linux tasks are uniquely identified
// at a given time by their PID, but those PIDs may be reused after hitting the
// maximum PID value.
type Task struct {
	// PID is the kernel's internal process identifier, which is equivalent
	// to the TID in userspace.
	PID int

	// TGID is the kernel's internal thread group identifier, which is
	// equivalent to the PID in userspace. All threads within a process
	// have differing PIDs, but all share the same TGID. The thread group
	// leader process's PID will be the same as its TGID.
	TGID int

	// Command is the kernel's comm field, which is initialized to the
	// first 15 characters of the basename of the executable being run.
	// It is also set via pthread_setname_np(3) and prctl(2) PR_SET_NAME.
	// It is always NULL-terminated and no longer than 16 bytes (including
	// NUL byte).
	Command string

	// CommandLine is the command-line used when the process was exec'd via
	// execve(). It is composed of the first 6 elements of argv. It may
	// not be complete if argv contained more than 6 elements.
	CommandLine []string

	// Creds are the credentials (uid, gid) for the task. This is kept
	// up-to-date by recording changes observed via a kprobe on
	// commit_creds().
	Creds *Cred

	// ContainerID is the ID of the container to which the task belongs,
	// if any.
	ContainerID string

	// ContainerInfo is a pointer to the cached container information for
	// the container to which the task belongs, if any.
	ContainerInfo *ContainerInfo

	// StartTime is the time at which a task started.
	StartTime int64

	// ExitTime is the time at which a task exited.
	ExitTime int64

	// ProcessID is a unique ID for the task.
	ProcessID string

	// CWD is the current working directory for the task. Tasks within a
	// process can each have their own independent CWD.
	CWD string

	// parent is an internal reference to the parent of this task, which
	// could be either the thread group leader or another process. Use
	// Parent() to get the parent of a container.
	parent *Task

	// pendingClone is used internally for tracking information about a
	// task clone executed by the clone(2) system call. In kernels >= 3.9
	// this is not necessary
	pendingClone *cloneEvent

	// funkyExecTime is the time at which an exec event last occurred for
	// this task's thread group by one of its member tasks (but not itself)
	// If we see a process exit within a few ms of this, suppress it and
	// then clear this.
	funkyExecTime int64
}

var rootTask = Task{}

func newTask(pid int) *Task {
	return &Task{
		PID: pid,
	}
}

var (
	sensorPID     int
	sensorPIDOnce sync.Once
)

// IsSensor returns true if the task belongs to the sensor process.
func (t *Task) IsSensor() bool {
	return t.TGID == sensorPID
}

// Leader returns a reference to a task's leader task.
func (t *Task) Leader() *Task {
	if t.PID == t.TGID {
		return t
	}
	return t.Parent()
}

// Parent returns a reference to a task's parent task.
func (t *Task) Parent() *Task {
	if t.parent == nil {
		// t.parent may not be set yet if events arrive out of order.
		if t.TGID != 0 {
			glog.Fatalf("Internal error: t.PID == %d && t.TGID == %d && t.parent == nil", t.PID, t.TGID)
		}
		// We don't know anything about the parent, so return this task
		// as the parent. Don't store the reference, though.
		return t
	}
	return t.parent
}

func (t *Task) suppressExitEvent(ts int64) bool {
	if t.funkyExecTime <= 0 {
		return false
	}
	var delta int64
	if t.funkyExecTime > ts {
		delta = t.funkyExecTime - ts
	} else {
		delta = ts - t.funkyExecTime
	}
	return delta <= taskReuseThreshold
}

type taskCache interface {
	LookupTask(int) *Task
}

type arrayTaskCache struct {
	entries []*Task
}

func newArrayTaskCache(size uint) *arrayTaskCache {
	return &arrayTaskCache{
		entries: make([]*Task, size),
	}
}

func (c *arrayTaskCache) LookupTask(pid int) *Task {
	if pid <= 0 || pid > len(c.entries) {
		glog.Fatalf("Invalid pid for lookup: %d", pid)
	}

	var t *Task
	for {
		var old *Task

		t = c.entries[pid-1]
		if t != nil {
			if t.ExitTime == 0 ||
				sys.CurrentMonotonicRaw()-t.ExitTime <
					taskReuseThreshold {
				break
			}
			old = t
		}
		t = newTask(pid)
		if atomic.CompareAndSwapPointer(
			(*unsafe.Pointer)(unsafe.Pointer(&c.entries[pid-1])),
			unsafe.Pointer(old),
			unsafe.Pointer(t)) {
			break
		}
	}
	glog.V(10).Infof("LookupTask(%d) -> %+v", pid, t)
	return t
}

type mapTaskCache struct {
	sync.Mutex
	entries map[int]*Task
}

func newMapTaskCache(size uint) *mapTaskCache {
	// The size here is likely to be quite large, so we don't really want
	// to size the map to it initially. On the other hand, we don't want
	// to let the map grow naturally using Go defaults, because we assume
	// that we will have a fairly large number of tasks.
	return &mapTaskCache{
		entries: make(map[int]*Task, size/4),
	}
}

func (c *mapTaskCache) LookupTask(pid int) *Task {
	if pid <= 0 {
		glog.Fatalf("Invalid pid for lookup: %d", pid)
	}

	c.Lock()
	t, ok := c.entries[pid]
	if !ok || (t.ExitTime != 0 &&
		sys.CurrentMonotonicRaw()-t.ExitTime >= taskReuseThreshold) {
		t = newTask(pid)
		c.entries[pid] = t
	}
	c.Unlock()
	glog.V(10).Infof("LookupTask(%d) -> %+v", pid, t)
	return t
}

// ProcessInfoCache is an object that caches process information. It is
// maintained automatically via an existing sensor object.
type ProcessInfoCache struct {
	cache taskCache

	sensor *Sensor

	// These are external event IDs registered with the sensor's event
	// monitor instance. The cache will enqueue these events as appropriate
	// as the cache is updated.
	ProcessExecEventID   uint64
	ProcessForkEventID   uint64
	ProcessExitEventID   uint64
	ProcessUpdateEventID uint64

	// EventGroupID is the EventMonitor event group id to use for
	// registering all process related events.
	EventGroupID int32

	startLock  sync.Mutex
	startQueue []scannerDeferredAction
	started    bool
}

type scannerDeferredAction func()

// NewProcessInfoCache creates a new process information cache object. An
// existing sensor object is required in order for the process info cache to
// able to install its probes to monitor the system to maintain the cache.
func NewProcessInfoCache(sensor *Sensor) *ProcessInfoCache {
	cache := &ProcessInfoCache{
		sensor: sensor,
	}

	sensorPIDOnce.Do(func() {
		sensorPID = sensor.ProcFS.SelfTGID()
		glog.V(1).Infof("Filtering all events from kernel TGID %d",
			sensorPID)
	})

	maxPID := sensor.ProcFS.MaxPID()
	if maxPID > config.Sensor.ProcessInfoCacheSize {
		cache.cache = newMapTaskCache(maxPID)
	} else {
		cache.cache = newArrayTaskCache(maxPID)
	}

	monitor := sensor.Monitor()

	var err error
	cache.EventGroupID, err = monitor.RegisterEventGroup("tasks",
		cache.lostRecordHandler)
	if err != nil {
		glog.Fatalf("Failed to register task event group: %v", err)
	}

	cache.ProcessExecEventID = monitor.ReserveEventID()
	cache.ProcessForkEventID = monitor.ReserveEventID()
	cache.ProcessExitEventID = monitor.ReserveEventID()
	cache.ProcessUpdateEventID = monitor.ReserveEventID()

	// Register with the sensor's global event monitor...
	if err = cache.installForkMonitor(); err != nil {
		glog.Fatalf("Couldn't register fork handlers: %v", err)
	}

	eventName := doExitAddress
	_, err = sensor.RegisterKprobe(eventName, false,
		doExitArgs, cache.handleDoExit, cache.EventGroupID,
		perf.WithTracingEventName("doexit"),
		perf.WithEventEnabled())
	if err != nil {
		glog.Fatalf("Couldn't register event %s: %s", eventName, err)
	}

	// Attach kprobe on commit_creds to capture task privileges
	_, err = sensor.RegisterKprobe(commitCredsAddress, false,
		commitCredsArgs, cache.handleCommitCreds, cache.EventGroupID,
		perf.WithTracingEventName("creds"),
		perf.WithEventEnabled())

	// Attach kretprobe on set_fs_pwd to track working directories
	_, err = sensor.RegisterKprobe(doSetFsPwd, true,
		"", cache.handleDoSetFsPwd, cache.EventGroupID,
		perf.WithTracingEventName("setfspwd"),
		perf.WithEventEnabled())

	// Attach a probe to capture exec events in the kernel. There are three
	// possibilities, in descending order of preference:
	//      __do_execve_file [Linux 4.18+]
	//      do_execveat_common [Linux 3.19-4.17]
	//      do_execve_common(const char *, ...) [Linux 3.0-3.13]
	//	do_execve_common(struct filename*, ...) [Linux 3.14-3.18]
	//	do_execve
	//
	// For do_execve_common, check to see if the symbol do_open_exec exists.
	// If it does, use struct filename * instead of const char *.
	//
	var execveProbes = []struct {
		address          string
		args             [][2]string
		compatRegister   string
		nocompatRegister string
	}{
		{
			address: doExecvefileAddress,
			args: [][2]string{
				{"", doExecvefileArgs},
			},
			compatRegister:   "cx",
			nocompatRegister: "dx",
		},
		{
			address: doExecveatCommonAddress,
			args: [][2]string{
				{"", doExecveatCommonArgs},
			},
			compatRegister:   "cx",
			nocompatRegister: "dx",
		},
		{
			address: doExecveCommonAddress,
			args: [][2]string{
				{"do_open_exec", doExecveCommonStructFilenameArgs},
				{"", doExecveCommonConstCharArgs},
			},
			compatRegister:   "dx",
			nocompatRegister: "di",
		},
		{
			address: doExecveAddress,
			args: [][2]string{
				{"", doExecveArgs},
			},
			compatRegister:   "si",
			nocompatRegister: "si",
		},
	}
	compatMode := sensor.IsKernelSymbolAvailable("compat_sys_execve") ||
		sensor.IsKernelSymbolAvailable("__ia32_compat_sys_execve")
	for i, probe := range execveProbes {
		var execveArgs string
		if compatMode {
			execveArgs = makeExecveFetchArgs(probe.compatRegister)
		} else {
			execveArgs = makeExecveFetchArgs(probe.nocompatRegister)
		}

		var args string
		for _, choice := range probe.args {
			if choice[0] == "" || sensor.IsKernelSymbolAvailable(choice[0]) {
				args = choice[1]
				break
			}
		}

		eventName = fmt.Sprintf("execve%d", i+1)
		_, err = sensor.RegisterKprobe(probe.address, false,
			args+execveArgs,
			cache.handleExecve,
			cache.EventGroupID,
			perf.WithTracingEventName(eventName),
			perf.WithEventEnabled())
		if err == nil {
			break
		}
	}
	if err != nil {
		glog.Fatalf("Could not install exec kprobe (tried %s, %s, %s)",
			doExecveatCommonAddress, doExecveCommonAddress,
			doExecveAddress)
	}

	if err = cache.installCgroupMonitor(); err != nil {
		glog.Fatalf("Could not install cgroup monitoring: %v", err)
	}

	return cache
}

func makeExecveFetchArgs(reg string) string {
	parts := make([]string, execveArgCount)
	for i := 0; i < execveArgCount; i++ {
		parts[i] = fmt.Sprintf("argv%d=+0(+%d(%%%s)):string", i, i*8, reg)
	}
	return strings.Join(parts, " ")
}

// Start enables the process cache by scanning the /proc filesystem to learn
// about existing processes and enable monitoring once that is done.
func (pc *ProcessInfoCache) Start() {
	if pc.started {
		glog.Fatal("Illegal second call to ProcessInfoCache.Start()")
	}

	// Scan the proc filesystem to learn about all existing tasks.
	if err := pc.scanProcFilesystem(); err != nil {
		glog.Fatal(err)
	}

	pc.startLock.Lock()
	for len(pc.startQueue) > 0 {
		queue := pc.startQueue
		pc.startQueue = nil
		pc.startLock.Unlock()
		for _, f := range queue {
			f()
		}
		pc.startLock.Lock()
	}
	pc.started = true
	pc.startLock.Unlock()
}

func (pc *ProcessInfoCache) cacheTaskFromProc(tgid, pid int) error {
	var s struct {
		Name string   `Name`
		PID  int      `Pid`
		PPID int      `PPid`
		TGID int      `Tgid`
		UID  []uint32 `Uid`
		GID  []uint32 `Gid`
	}
	procFS := pc.sensor.ProcFS
	err := procFS.ReadTaskStatus(tgid, pid, &s)
	if err != nil {
		return fmt.Errorf("Couldn't read pid %d status: %s",
			pid, err)
	}
	tgid = s.TGID // this may change if we're caching for a missed fork
	glog.V(2).Infof("Found task %d (tgid %d): %s", pid, tgid, s.Name)

	t := pc.cache.LookupTask(s.PID)
	t.TGID = s.TGID
	t.Command = s.Name
	t.Creds = newCredentials(s.UID[0], s.UID[1], s.UID[2], s.UID[3],
		s.GID[0], s.GID[1], s.GID[2], s.GID[3])
	t.StartTime, err = procFS.TaskStartTime(tgid, pid)
	if err != nil {
		t.StartTime = sys.CurrentMonotonicRaw()
	}
	t.ProcessID = procFS.TaskUniqueID(t.TGID, t.PID, t.StartTime)
	if t.PID != t.TGID {
		// TGID should never be 0 in theory, but it could be if the task
		// is in the process of exiting and we happened to catch it at
		// just the right time. If we call LookupTask with a pid of 0,
		// it will panic.
		if t.TGID != 0 {
			t.parent = pc.cache.LookupTask(t.TGID)
		}
	} else {
		if s.PPID == 0 && (t.PID == 1 || s.Name == "kthreadd") {
			t.parent = &rootTask
		} else {
			t.parent = pc.cache.LookupTask(s.PPID)
		}
		// Failures here can be ignored; the task has completed while
		// we've been processing it.
		t.CommandLine, _ = procFS.ProcessCommandLine(t.TGID)
		t.CWD, _ = procFS.TaskCWD(t.TGID, t.PID)
		t.ContainerID, _ = procFS.ProcessContainerID(tgid)
	}

	return nil
}

func (pc *ProcessInfoCache) scanProcFilesystem() error {
	return pc.sensor.ProcFS.WalkTasks(func(tgid, pid int) bool {
		pc.cacheTaskFromProc(tgid, pid)
		return true
	})
}

func (pc *ProcessInfoCache) installCgroupMonitor() error {
	// All kernel versions have the "cgroup_procs_write" symbol, which we
	// will use to determine whether cgroup monitoring is possible at all.
	// Structs are different in different kernel versions, though, so we
	// have to do more work to pick out the right symbol and fetchargs.
	if !pc.sensor.IsKernelSymbolAvailable("cgroup_procs_write") {
		glog.Info("CGROUP monitoring disabled: kernel compiled without CGROUPS support")
		return nil
	}

	var (
		eventName, fetchArgs   string
		eventName2, fetchArgs2 string
	)

	// attach_task_by_pid exists in kernel versions < 3.16
	if pc.sensor.IsKernelSymbolAvailable(attachTaskByPid) {
		eventName = attachTaskByPid

		// The structure of struct cgroup changes in 3.5. The symbol
		// cgroup_add_files exists in kernel versions < 3.5.
		// The structure of struct cgroup changes again in 3.15. The
		// symbol cgroup_taskset_size exists in kernel versions < 3.15.
		if pc.sensor.IsKernelSymbolAvailable("cgroup_add_files") {
			fetchArgs = attachTaskByPidArgs1
		} else if pc.sensor.IsKernelSymbolAvailable("cgroup_taskset_size") {
			fetchArgs = attachTaskByPidArgs2
		} else {
			fetchArgs = attachTaskByPidArgs3
		}
	} else if pc.sensor.IsKernelSymbolAvailable(cgroupProcsWriteAddress) {
		eventName = cgroupProcsWriteAddress
		fetchArgs = cgroupProcsWriteArgs
	} else if pc.sensor.IsKernelSymbolAvailable(cgroup1ProcsWriteAddress) {
		eventName = cgroup1ProcsWriteAddress
		fetchArgs = cgroup1ProcsWriteArgs
		eventName2 = "cgroup_procs_write"
		fetchArgs2 = cgroupProcsWriteArgs
	} else {
		return errors.New("Known cgroup symbols not found")
	}

	_, err := pc.sensor.RegisterKprobe(eventName, false,
		fetchArgs, pc.handleCgroupProcsWrite, pc.EventGroupID,
		perf.WithTracingEventName("cgroups1"),
		perf.WithEventEnabled())
	if err != nil {
		return fmt.Errorf("%s: %v", eventName, err)
	}

	if eventName2 != "" {
		_, err = pc.sensor.RegisterKprobe(eventName2, false,
			fetchArgs2, pc.handleCgroupProcsWrite, pc.EventGroupID,
			perf.WithTracingEventName("cgroups2"),
			perf.WithEventEnabled())
		if err != nil {
			return fmt.Errorf("%s: %v", eventName2, err)
		}
	}

	return nil
}

func (pc *ProcessInfoCache) replaceTaskStructOffsets(s string) string {
	s = strings.Replace(s, "PID_OFFSET", fmt.Sprintf("%d", pc.sensor.taskStructPID.Offset), -1)
	s = strings.Replace(s, "TGID_OFFSET", fmt.Sprintf("%d", pc.sensor.taskStructTGID.Offset), -1)
	s = strings.Replace(s, "START_TIME_SEC_OFFSET", fmt.Sprintf("%d", pc.sensor.taskStructRealStartTime.Offset), -1)
	s = strings.Replace(s, "START_TIME_NSEC_OFFSET", fmt.Sprintf("%d", pc.sensor.taskStructRealStartTime.Offset+8), -1)
	return s
}

func (pc *ProcessInfoCache) installForkMonitor() error {
	monitor := pc.sensor.Monitor()

	// Use the preferred method if we have struct task_struct offsets for
	// pid, tgid, and start_time.
	if pc.sensor.taskStructPID.Size != 0 &&
		(pc.sensor.taskStructRealStartTime.Size == 8 ||
			pc.sensor.taskStructRealStartTime.Size == 16) {
		// Install kprobes for doForkAddress and wakeUpNewTaskAddress
		// If this fails for any reason, fall back to the same behavior
		// as when no offsets are available.
		args := pc.replaceTaskStructOffsets(doForkFetchargs)
		eventid, err := pc.sensor.RegisterKprobe(doForkAddress,
			false, args, pc.handleDoFork, pc.EventGroupID,
			perf.WithTracingEventName("dofork"),
			perf.WithEventEnabled())
		if err != nil {
			eventid, err = pc.sensor.RegisterKprobe(
				"_"+doForkAddress, false, args, pc.handleDoFork,
				pc.EventGroupID,
				perf.WithTracingEventName("dofork"),
				perf.WithEventEnabled())
		}
		if err == nil {
			switch pc.sensor.taskStructRealStartTime.Size {
			case 16:
				args = wakeUpNewTaskRealStartTime16Fetchargs
			case 8:
				args = wakeUpNewTaskRealStartTime8Fetchargs
			}
			args = pc.replaceTaskStructOffsets(args)
			_, err = pc.sensor.RegisterKprobe(wakeUpNewTaskAddress,
				false, args, pc.handleWakeUpNewTask,
				pc.EventGroupID,
				perf.WithTracingEventName("wake_up_new_task"),
				perf.WithEventEnabled())
			if err == nil {
				glog.V(2).Info("Using task_struct offsets for fork tracing")
				return nil
			}
			monitor.UnregisterEvent(eventid)
		}
	}

	eventName := doForkAddress
	_, err := pc.sensor.RegisterKprobe(eventName, false,
		doForkFetchargs, pc.handleDoFork, pc.EventGroupID,
		perf.WithTracingEventName("dofork"),
		perf.WithEventEnabled())
	if err != nil {
		eventName = "_" + eventName
		_, err = pc.sensor.RegisterKprobe(eventName, false,
			doForkFetchargs, pc.handleDoFork, pc.EventGroupID,
			perf.WithTracingEventName("_dofork"),
			perf.WithEventEnabled())
		if err != nil {
			glog.Fatalf("Couldn't register kprobe %s: %s",
				eventName, err)
		}
	}

	eventName = "sched/sched_process_fork"
	_, err = monitor.RegisterTracepoint(eventName,
		pc.handleSchedProcessFork, pc.EventGroupID,
		perf.WithEventEnabled())
	if err != nil {
		glog.Fatalf("Couldn't register kprobe %s: %s",
			eventName, err)
	}

	return nil
}

func (pc *ProcessInfoCache) lostRecordHandler(
	eventid uint64,
	groupid int32,
	sampleID perf.SampleID,
	count uint64,
) {
	glog.V(1).Infof("Lost %d process related events\n", count)

	var e LostRecordTelemetryEvent
	e.InitWithSampleID(pc.sensor, sampleID, count)
	e.Type = LostRecordTypeProcess
	pc.sensor.DispatchEventToAllSubscriptions(e)

	atomic.AddUint64(&pc.sensor.Metrics.KernelSamplesLost, count)
}

// LookupTask finds the task information for the given PID.
func (pc *ProcessInfoCache) LookupTask(pid int) *Task {
	t := pc.cache.LookupTask(pid)
	if t.TGID == 0 {
		// The fork for this process was lost. We don't know what the
		// TGID actually is, but procfs has a little quirk that means
		// we don't really need it. Listing files in /proc will yield
		// only TGIDs, but any valid PID is recognized as a legit path
		pc.cacheTaskFromProc(t.PID, t.PID)
	}
	return t
}

// LookupTaskAndLeader finds the task information for both a given PID and the
// thread group leader.
func (pc *ProcessInfoCache) LookupTaskAndLeader(pid int) (*Task, *Task) {
	t := pc.cache.LookupTask(pid)
	if t.TGID == 0 {
		// See comment in LookupTask above
		pc.cacheTaskFromProc(t.PID, t.PID)
	}
	l := t.Leader()
	if l.TGID == 0 {
		// See comment in LookupTask above
		pc.cacheTaskFromProc(l.PID, l.PID)
	}
	return t, l
}

// LookupTaskContainerInfo returns the container info for a task, possibly
// consulting the sensor's container cache and updating the task cached
// information.
func (pc *ProcessInfoCache) LookupTaskContainerInfo(t *Task) *ContainerInfo {
	if t.PID != t.TGID {
		t = t.Parent()
	}
	if i := t.ContainerInfo; i != nil {
		return i
	}

	if ID := t.ContainerID; len(ID) > 0 {
		if i := pc.sensor.ContainerCache.LookupContainer(ID, true); i != nil {
			t.ContainerInfo = i
			return i
		}
	}

	return nil
}

func (pc *ProcessInfoCache) maybeDeferAction(f func()) {
	if !pc.started {
		pc.startLock.Lock()
		if !pc.started {
			pc.startQueue = append(pc.startQueue, f)
			pc.startLock.Unlock()
			return
		}
		pc.startLock.Unlock()
	}

	f()
}

func (pc *ProcessInfoCache) dispatchProcessExecEvent(
	sampleID perf.SampleID,
	data expression.FieldValueMap,
) {
	var e ProcessExecTelemetryEvent
	if e.InitWithSampleID(pc.sensor, sampleID) {
		e.Filename = data["filename"].(string)
		e.CommandLine = data["exec_command_line"].([]string)
		e.CWD = data["cwd"].(string)
		pc.sensor.DispatchEvent(pc.ProcessExecEventID, e, data)
	}
}

func (pc *ProcessInfoCache) dispatchProcessExitEvent(
	sampleID perf.SampleID,
	data expression.FieldValueMap,
) {
	var e ProcessExitTelemetryEvent
	if e.InitWithSampleID(pc.sensor, sampleID) {
		e.ExitCode = data["code"].(int32)
		e.ExitStatus = data["exit_status"].(uint32)
		e.ExitSignal = data["exit_signal"].(uint32)
		e.ExitCoreDumped = data["exit_core_dumped"].(bool)
		pc.sensor.DispatchEvent(pc.ProcessExitEventID, e, data)
	}
}

func (pc *ProcessInfoCache) dispatchProcessForkEvent(
	sampleID perf.SampleID,
	data expression.FieldValueMap,
) {
	var e ProcessForkTelemetryEvent
	if e.InitWithSampleID(pc.sensor, sampleID) {
		e.ChildPID = data["fork_child_pid"].(int32)
		e.ChildProcessID = data["fork_child_id"].(string)
		e.CloneFlags = data["fork_clone_flags"].(uint64)
		e.StackStart = data["fork_stack_start"].(uint64)
		e.CWD = data["cwd"].(string)
		pc.sensor.DispatchEvent(pc.ProcessForkEventID, e, data)
	}
}

func (pc *ProcessInfoCache) dispatchProcessUpdateEvent(
	sampleID perf.SampleID,
	data expression.FieldValueMap,
) {
	var e ProcessUpdateTelemetryEvent
	if e.InitWithSampleID(pc.sensor, sampleID) {
		e.CWD = data["cwd"].(string)
		pc.sensor.DispatchEvent(pc.ProcessUpdateEventID, e, data)
	}
}

func (pc *ProcessInfoCache) handleSysClone(
	parentTask, parentLeader, childTask *Task,
	cloneFlags uint64,
	childComm string,
	sampleID perf.SampleID,
	startTime int64,
	stackStart uint64,
) {
	childTask.parent = parentLeader
	childTask.Command = childComm
	childTask.Creds = parentTask.Creds

	if (cloneFlags & CLONE_THREAD) != 0 {
		childTask.TGID = parentLeader.TGID
	} else {
		// This is a new thread group leader, tgid is the new pid
		childTask.TGID = childTask.PID
	}
	childTask.ContainerID = parentLeader.ContainerID
	childTask.ContainerInfo = parentLeader.ContainerInfo

	procFS := pc.sensor.ProcFS
	if startTime != 0 {
		// Convert nsec to clock_t: startTime / NSEC_PER_SEC / USER_HZ
		// For x86_64 (and most arches it would seem): USER_HZ is 100
		// We convert because this is what fs/proc/array.c does when
		// producing /proc/<tgid>/task/<pid>/stat. We want to match
		// for start-up scans.
		childTask.StartTime = startTime / 1e7
	} else {
		var err error
		childTask.StartTime, err = procFS.TaskStartTime(childTask.TGID,
			childTask.PID)
		if err != nil {
			childTask.StartTime = int64(sampleID.Time)
		}
	}
	childTask.ProcessID = procFS.TaskUniqueID(childTask.TGID,
		childTask.PID, childTask.StartTime)

	eventData := expression.FieldValueMap{
		"fork_child_pid":   int32(childTask.PID),
		"fork_child_id":    childTask.ProcessID,
		"fork_clone_flags": cloneFlags,
		"fork_stack_start": stackStart,
		"cwd":              parentTask.CWD,
	}
	pc.dispatchProcessForkEvent(sampleID, eventData)
}

func (pc *ProcessInfoCache) prepareTaskExit(exitCode int64) expression.FieldValueMap {
	ws := unix.WaitStatus(exitCode)
	eventData := expression.FieldValueMap{
		"code": int32(exitCode),
	}
	if ws.Exited() {
		eventData["exit_status"] = uint32(ws.ExitStatus())
		eventData["exit_signal"] = uint32(0)
		eventData["exit_core_dumped"] = false
	} else if ws.Signaled() {
		eventData["exit_status"] = uint32(0)
		eventData["exit_signal"] = uint32(ws.Signal())
		eventData["exit_core_dumped"] = ws.CoreDump()
	}

	return eventData
}

// handleDoExit marks a task as having exited. The time of the exit
// is noted so that the task can be safely reused later. Don't delete it from
// the cache, because out-of-order events could cause it to be recreated and
// then when the pid is reused by the kernel later, it'll contain stale data.
func (pc *ProcessInfoCache) handleDoExit(_ uint64, sample *perf.Sample) {
	exitCode, _ := sample.GetSignedInt64("code")
	exitTime := sys.CurrentMonotonicRaw()

	eventData := pc.prepareTaskExit(exitCode)

	// references to sample cannot outlive this function, because
	// the sample may be reused in EventMonitor.
	sampleID := sample.SampleID
	pc.maybeDeferAction(func() {
		t := pc.LookupTask(int(sampleID.TID))
		if t.suppressExitEvent(int64(sample.Time)) {
			t.funkyExecTime = 0
			return
		}
		pc.dispatchProcessExitEvent(sampleID, eventData)
		t.ExitTime = exitTime
	})
}

func (pc *ProcessInfoCache) handleCommitCreds(_ uint64, sample *perf.Sample) {
	pid := int(sample.TID)
	newCreds := &Cred{}
	newCreds.UID, _ = sample.GetUnsignedInt32("uid")
	newCreds.GID, _ = sample.GetUnsignedInt32("gid")
	newCreds.EUID, _ = sample.GetUnsignedInt32("euid")
	newCreds.EGID, _ = sample.GetUnsignedInt32("egid")
	newCreds.SUID, _ = sample.GetUnsignedInt32("suid")
	newCreds.SGID, _ = sample.GetUnsignedInt32("sgid")
	newCreds.FSUID, _ = sample.GetUnsignedInt32("fsuid")
	newCreds.FSGID, _ = sample.GetUnsignedInt32("fsgid")

	pc.maybeDeferAction(func() {
		t := pc.LookupTask(pid)
		t.Creds = newCreds
	})
}

func (pc *ProcessInfoCache) handleDoSetFsPwd(_ uint64, sample *perf.Sample) {
	pid := int(sample.TID)

	// references to sample cannot outlive this function, because
	// the sample may be reused in EventMonitor.
	sampleID := sample.SampleID
	pc.maybeDeferAction(func() {
		t := pc.LookupTask(pid)
		cwd, err := pc.sensor.ProcFS.TaskCWD(t.TGID, t.PID)
		if err == nil && cwd != t.CWD {
			glog.V(10).Infof("CWD(%d) = %s", t.PID, cwd)
			t.CWD = cwd

			eventData := expression.FieldValueMap{
				"cwd": t.CWD,
			}
			pc.dispatchProcessUpdateEvent(sampleID, eventData)
		}
	})
}

// handleDoExecve handles sys_execve() and sys_execveat() events to obtain the
// command-line for the process.
func (pc *ProcessInfoCache) handleExecve(_ uint64, sample *perf.Sample) {
	pid := int(sample.TID)
	command, _ := sample.GetString("filename")
	commandLine := make([]string, 0, execveArgCount)
	for i := 0; i < execveArgCount; i++ {
		s, _ := sample.GetString(execveArgString[i])
		if len(s) == 0 {
			break
		}
		commandLine = append(commandLine, s)
	}

	eventData := expression.FieldValueMap{
		"filename":          command,
		"exec_command_line": commandLine,
	}

	// references to sample cannot outlive this function, because
	// the sample may be reused in EventMonitor.
	sampleID := sample.SampleID
	pc.maybeDeferAction(func() {
		t := pc.LookupTask(pid)
		if t.PID != t.TGID {
			execTime := int64(sampleID.Time)

			// Send a process exit for this task
			exitEventData := pc.prepareTaskExit(0)
			pc.dispatchProcessExitEvent(sampleID, exitEventData)
			t.ExitTime = execTime

			// This task becomes the task leader. Note that the
			// kernel updates tsk->start_time to be leader->start_time,
			// but it does not update real_start_time, which is what
			// procfs uses, so we don't update start time here either
			sampleID.TID = uint32(t.TGID)
			l := t.Leader()
			l.Creds = t.Creds
			l.ContainerID = t.ContainerID
			l.ContainerInfo = t.ContainerInfo
			l.ExitTime = 0
			l.CWD = t.CWD
			l.funkyExecTime = execTime
			t = l
		}

		t.Command = command
		t.CommandLine = commandLine
		eventData["cwd"] = t.CWD
		pc.dispatchProcessExecEvent(sampleID, eventData)
	})
}

func (pc *ProcessInfoCache) handleDoFork(_ uint64, sample *perf.Sample) {
	pid := int(sample.TID)
	cloneFlags, _ := sample.GetUnsignedInt64("clone_flags")
	stackStart, _ := sample.GetUnsignedInt64("stack_start")

	// references to sample cannot outlive this function, because
	// the sample may be reused in EventMonitor.
	sampleID := sample.SampleID
	pc.maybeDeferAction(func() {
		t := pc.LookupTask(pid)

		if t.pendingClone.isExpired(sample.Time) {
			t.pendingClone = &cloneEvent{
				timestamp:  sampleID.Time,
				cloneFlags: cloneFlags,
				stackStart: stackStart,
			}
		} else if t.pendingClone.childPid != 0 {
			c := t.pendingClone
			t.pendingClone = nil

			childTask := pc.cache.LookupTask(c.childPid)
			pc.handleSysClone(t, t.Leader(), childTask,
				cloneFlags, c.childComm, sampleID, 0, stackStart)
		}
	})
}

func (pc *ProcessInfoCache) handleSchedProcessFork(_ uint64, sample *perf.Sample) {
	parentPid, _ := sample.GetSignedInt32("parent_pid")
	childPid, _ := sample.GetSignedInt32("child_pid")

	commValue, _ := sample.DecodeValue("child_comm")
	childComm := commToString(commValue)

	// references to sample cannot outlive this function, because
	// the sample may be reused in EventMonitor.
	sampleID := sample.SampleID
	pc.maybeDeferAction(func() {
		parentTask, parentLeader := pc.LookupTaskAndLeader(int(parentPid))
		if parentTask.pendingClone.isExpired(sampleID.Time) {
			parentTask.pendingClone = &cloneEvent{
				timestamp: sampleID.Time,
				childPid:  int(childPid),
				childComm: childComm,
			}
		} else {
			c := parentTask.pendingClone
			parentTask.pendingClone = nil

			childTask := pc.cache.LookupTask(int(childPid))
			pc.handleSysClone(parentTask, parentLeader, childTask,
				c.cloneFlags, childComm, sampleID, 0,
				c.stackStart)
		}
	})
}

func (pc *ProcessInfoCache) handleWakeUpNewTask(_ uint64, sample *perf.Sample) {
	parentPid := int(sample.TID)
	childPid, _ := sample.GetSignedInt32("child_pid")

	var startTime int64
	startTimeSec, _ := sample.GetSignedInt64("start_time_sec")
	if startTimeNsec, err := sample.GetSignedInt64("start_time_nsec"); err == nil {
		startTime = (startTimeSec * 1e9) + startTimeNsec
	} else {
		startTime = startTimeSec
	}

	// references to sample cannot outlive this function, because
	// the sample may be reused in EventMonitor.
	sampleID := sample.SampleID
	pc.maybeDeferAction(func() {
		parentTask, parentLeader := pc.LookupTaskAndLeader(parentPid)
		if parentTask.pendingClone.isExpired(sampleID.Time) {
			parentTask.pendingClone = &cloneEvent{
				timestamp: sampleID.Time,
				childPid:  int(childPid),
				childComm: parentTask.Command,
			}
		} else {
			c := parentTask.pendingClone
			parentTask.pendingClone = nil

			childTask := pc.cache.LookupTask(int(childPid))
			pc.handleSysClone(parentTask, parentLeader, childTask,
				c.cloneFlags, parentTask.Command, sampleID,
				startTime, c.stackStart)
		}
	})
}

func (pc *ProcessInfoCache) handleCgroupProcsWrite(_ uint64, sample *perf.Sample) {
	// We're looking for container IDs, which are normally hex encoded
	// sha256 hashes. Basically just ensure that we've got exactly 64
	// characters. That should be good enough for now.
	containerID, _ := sample.GetString("container_id")
	if containerID = sys.ContainerID(containerID); containerID == "" {
		return
	}

	var (
		buf         string
		err         error
		pid         int
		threadgroup bool
		vs32        int32
		vs64        int64
		vu64        uint64
	)

	if buf, err = sample.GetString("buf"); err == nil {
		buf = strings.TrimSpace(buf)
		if vs64, err = strconv.ParseInt(buf, 10, 32); err == nil {
			pid = int(vs64)
		} else {
			// If there's an error parsing data written to this file, we
			// want to know about it so that we can deal with it. Use a
			// high logging level to ensure that.
			glog.Warningf("Error parsing pid written to cgroup.procs: %v", err)
			return nil, nil
		}
	} else if vu64, err = sample.GetUnsignedInt64("tgid"); err == nil {
		pid = int(vu64)
		threadgroup = true
	} else if vu64, err = sample.GetUnsignedInt64("pid"); err == nil {
		pid = int(vu64)
		threadgroup = false
	}
	if vs32, err = sample.GetSignedInt32("threadgroup"); err == nil && vs32 != 0 {
		threadgroup = true
	}

	pc.maybeDeferAction(func() {
		task, leader := pc.LookupTaskAndLeader(int(pid))
		if threadgroup {
			task = leader
		}

		// This event fires once for each cgroup a task is entered
		// into, which typically 13 times, so try to minimize the
		// amount of work done
		if task.ContainerID == containerID {
			return
		}

		glog.V(10).Infof("containerID(%d) = %s", task.PID, containerID)
		if containerID != task.ContainerID {
			task.ContainerID = containerID
			task.ContainerInfo = nil
		}
	})
}

func commToString(comm interface{}) string {
	// Most kernels report []int8, but 2.6.32 reports []uint8. Convert all
	// cases to []byte to make a string.
	var b []byte
	switch v := comm.(type) {
	case []int8:
		var slice = struct {
			addr     uintptr
			len, cap int
		}{uintptr(unsafe.Pointer(&v[0])), len(v), len(v)}
		b = *(*[]byte)(unsafe.Pointer(&slice))
	case []uint8:
		b = []byte(v)
	default:
		return ""
	}

	for i, x := range b {
		if x == 0 {
			b = b[:i]
			break
		}
	}
	return string(b)
}

func (s *Subscription) registerProcessEventFilter(
	eventID uint64,
	expr *expression.Expression,
) {
	if _, err := s.addEventSink(eventID, expr); err != nil {
		s.logStatus(
			fmt.Sprintf("Invalid process filter expression: %v", err))
	}
}

// RegisterProcessExecEventFilter registers a process exec event filter with a
// subscription.
func (s *Subscription) RegisterProcessExecEventFilter(expr *expression.Expression) {
	s.registerProcessEventFilter(
		s.sensor.ProcessCache.ProcessExecEventID, expr)
}

// RegisterProcessExitEventFilter registers a process exit event filter with a
// subscription.
func (s *Subscription) RegisterProcessExitEventFilter(expr *expression.Expression) {
	s.registerProcessEventFilter(
		s.sensor.ProcessCache.ProcessExitEventID, expr)
}

// RegisterProcessForkEventFilter registers a process fork event filter with a
// subscription.
func (s *Subscription) RegisterProcessForkEventFilter(expr *expression.Expression) {
	s.registerProcessEventFilter(
		s.sensor.ProcessCache.ProcessForkEventID, expr)
}

// RegisterProcessUpdateEventFilter registers a process update event filter
// with a subscription.
func (s *Subscription) RegisterProcessUpdateEventFilter(expr *expression.Expression) {
	s.registerProcessEventFilter(
		s.sensor.ProcessCache.ProcessUpdateEventID, expr)
}
